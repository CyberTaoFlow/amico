=== PF_RING-Aware AMICO ===

For details on PF_RING installation, 
see http://www.ntop.org/products/pf_ring/

Here is what worked for us on Debian Wheezy:

Download and untar PF_RING

Make sure you have these packages installed

libpcap-dev
libnuma-dev
bison
flex
byacc

You will also have to install your linux kernel headers. Then

$ cd <PF_RING-DIR>/kernel
$ make
# make install

$ cd <PF_RING-DIR>/userland/lib
$ ./configure
$ make
# make install

$ cd ../libpcap
$ ./configure
$ make
# make install
# ldconfig /usr/local/lib/

To compile pe_dump with the new pf_ring-aware libpcap, use Makefile.pfring

You should see something like this:

amico:~/amico/trunk/pe_dump$ make
gcc    -c -o pe_dump.o pe_dump.c
gcc    -c -o search.o search.c
gcc    -c -o lru-cache.o lru-cache.c
gcc    -c -o seq_list.o seq_list.c
gcc   -o pe_dump pe_dump.o search.o lru-cache.o seq_list.o -L /usr/local/lib/ -lpcap -lpthread

amico:~/amico/trunk/pe_dump$ ldd pe_dump
        linux-vdso.so.1 =>  (0x00007fff7a52c000)
        libpcap.so.1 => /usr/local/lib/libpcap.so.1 (0x00007f9f8f10c000)
        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f9f8eef0000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f9f8eb64000)
        libnuma.so.1 => /usr/lib/libnuma.so.1 (0x00007f9f8e959000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f9f8f381000)


To actually use PF_RING you need to load the kernel module, for example

# insmod <PF_RING-DIR>/kernel/pf_ring.ko transparent_mode=0

See the documentation at http://www.ntop.org/pf_ring/pf_ring-and-transparent-mode/ 
for more info on transparent_mode


You may also want to compile and install a PF_RING-aware NIC driver. This worked for us on Debian wheezy with PF_RING-6.0.1

$ cd PF_RING-6.0.1/drivers/PF_RING_aware/intel/igb/igb-5.2.5-zc/src
$ make
# make install

Then, load the driver and pf_ring

# rmmod igb
# rmmod pf_ring
# echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
# mkdir /mnt/huge
# mount -t hugetlbfs nodev /mnt/huge
# insmod PF_RING-6.0.1/kernel/pf_ring.ko transparent_mode=2
# insmod PF_RING-6.0.1/drivers/PF_RING_aware/intel/igb/igb-5.2.5-zc/src/igb.ko RSS=1,1,1,1,1,1,1,1
# sudo ifconfig ethX up
# ethtool -G ethX rx 4096

The steps above have been adapted from 
PF_RING-6.0.1/drivers/PF_RING_aware/intel/igb/igb-5.2.5-zc/src/load_driver.sh


=== Some other possible optimizations ===

Another thing I noticed is a possible problem with ksoftirqd "clogging" one single CPU

If you are running multiple instances of pe_dump 
(e.g., four instances, one per each of four 1Gbps NIC) 
you might need to explicilty play with the CPU affinity.

For example, you can have each NIC have affinity with a specific CPU core

# echo 00000001 > /proc/irq/123/smp_affinity
# echo 00000002 > /proc/irq/125/smp_affinity
# echo 00000004 > /proc/irq/128/smp_affinity
# echo 00000008 > /proc/irq/130/smp_affinity

where you need to replace 123, 125, etc., with your NICs' IRQ numbers.
To check if the interrupts are actually being distributed as you would like, you can
take a look at the interrupt counts in /proc/interrupts

You should also take a look at 
PF_RING-6.0.1/drivers/PF_RING_aware/intel/igb/igb-5.2.5-zc/scripts/set_irq_affinity


We also noticed that "pinning" each pe_dump process to a specific CPU does help
with performance. For example, you could use something like

# taskset -c -p  CPU#  PID 



